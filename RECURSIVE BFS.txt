{
	"Breadth-First Search (BFS)": {
		"prefix": "Breadth-First Search (BFS)",
		"body": [
			"from collections import deque",
			"",
			"graph = {",
			"    \"S\": [(\"A\", 2), (\"B\", 4)],",
			"    \"A\": [(\"S\", 2), (\"B\", 2), (\"C\", 3), (\"D\", 7)],",
			"    \"B\": [(\"S\", 4), (\"A\", 2), (\"C\", 1)],",
			"    \"C\": [(\"A\", 3), (\"B\", 1), (\"G\", 5)],",
			"    \"D\": [(\"A\", 7), (\"G\", 2)],",
			"    \"G\": [(\"C\", 5), (\"D\", 2)]",
			"}",
			"",
			"def bfs(start, goal):",
			"    queue = deque([(start, [start])])",
			"    visited = set()",
			"    ",
			"    while queue:",
			"        node, path = queue.popleft()",
			"        ",
			"        if node == goal:",
			"            return path",
			"            ",
			"        if node not in visited:",
			"            visited.add(node)",
			"            for neighbor, _ in graph[node]:",
			"                if neighbor not in visited:",
			"                    queue.append((neighbor, path + [neighbor]))",
			"                    ",
			"    return None",
			"",
			"print(\"BFS Path:\", bfs(\"S\", \"G\"))"
		],
		"description": "Implements the Breadth-First Search (BFS) algorithm."
	},
	"Iterative Depth-First Search (DFS)": {
		"prefix": "Iterative Depth-First Search (DFS)",
		"body": [
			"graph = {",
			"    \"S\": [(\"A\", 2), (\"B\", 4)],",
			"    \"A\": [(\"S\", 2), (\"B\", 2), (\"C\", 3), (\"D\", 7)],",
			"    \"B\": [(\"S\", 4), (\"A\", 2), (\"C\", 1)],",
			"    \"C\": [(\"A\", 3), (\"B\", 1), (\"G\", 5)],",
			"    \"D\": [(\"A\", 7), (\"G\", 2)],",
			"    \"G\": [(\"C\", 5), (\"D\", 2)]",
			"}",
			"",
			"def dfs_iter(start, goal):",
			"    stack = [(start, [start])]",
			"    visited = set()",
			"    while stack:",
			"        node, path = stack.pop()",
			"        if node == goal:",
			"            return path",
			"        if node not in visited:",
			"            visited.add(node)",
			"            for neighbor, _ in graph[node]:",
			"                stack.append((neighbor, path + [neighbor]))",
			"    return None",
			"",
			"print(\"DFS Iterative Path:\", dfs_iter(\"S\", \"G\"))"
		],
		"description": "Implements an iterative Depth-First Search (DFS) algorithm."
	},
	"A* Search": {
		"prefix": "A* Search",
		"body": [
			"import heapq",
			"",
			"graph = {",
			"    \"S\": [(\"A\", 2), (\"B\", 4)],",
			"    \"A\": [(\"S\", 2), (\"B\", 2), (\"C\", 3), (\"D\", 7)],",
			"    \"B\": [(\"S\", 4), (\"A\", 2), (\"C\", 1)],",
			"    \"C\": [(\"A\", 3), (\"B\", 1), (\"G\", 5)],",
			"    \"D\": [(\"A\", 7), (\"G\", 2)],",
			"    \"G\": [(\"C\", 5), (\"D\", 2)]",
			"}",
			"",
			"heuristic = {",
			"    \"S\": 7, \"A\": 6, \"B\": 2, \"C\": 1, \"D\": 1, \"G\": 0",
			"}",
			"",
			"def astar(start, goal):",
			"    pq = [(heuristic[start], 0, start, [start])] # (f, g, node, path)",
			"    visited = set()",
			"    while pq:",
			"        f, g, node, path = heapq.heappop(pq)",
			"        if node == goal:",
			"            return path, g",
			"        if node not in visited:",
			"            visited.add(node)",
			"            for neighbor, cost in graph[node]:",
			"                new_g = g + cost",
			"                new_f = new_g + heuristic[neighbor]",
			"                heapq.heappush(pq, (new_f, new_g, neighbor, path + [neighbor]))",
			"    return None, float(\"inf\")",
			"",
			"path, cost = astar(\"S\", \"G\")",
			"print(\"A* Path:\", path, \"Cost:\", cost)"
		],
		"description": "Implements the A* search algorithm."
	},
	"Recursive Best-First Search (RBFS)": {
		"prefix": "Recursive Best-First Search (RBFS)",
		"body": [
			"graph = {",
			"    \"S\": [(\"A\", 2), (\"B\", 4)],",
			"    \"A\": [(\"S\", 2), (\"B\", 2), (\"C\", 3), (\"D\", 7)],",
			"    \"B\": [(\"S\", 4), (\"A\", 2), (\"C\", 1)],",
			"    \"C\": [(\"A\", 3), (\"B\", 1), (\"G\", 5)],",
			"    \"D\": [(\"A\", 7), (\"G\", 2)],",
			"    \"G\": [(\"C\", 5), (\"D\", 2)]",
			"}",
			"",
			"heuristic = {",
			"    \"S\": 7, \"A\": 6, \"B\": 2, \"C\": 1, \"D\": 1, \"G\": 0",
			"}",
			"",
			"def rbfs(node, path, g, f_limit, goal):",
			"    if node == goal:",
			"        return path, g",
			"    successors = []",
			"    for neighbor, cost in graph[node]:",
			"        if neighbor not in path:",
			"            new_g = g + cost",
			"            new_f = max(new_g + heuristic[neighbor], f_limit)",
			"            successors.append([new_f, neighbor, path + [neighbor], new_g])",
			"    if not successors:",
			"        return None, float(\"inf\")",
			"    while True:",
			"        successors.sort(key=lambda x: x[0])",
			"        best = successors[0]",
			"        if best[0] > f_limit:",
			"            return None, best[0]",
			"        alternative = successors[1][0] if len(successors) > 1 else float(\"inf\")",
			"        result, best[0] = rbfs(best[1], best[2], best[3], min(f_limit, alternative), goal)",
			"        successors[0] = best",
			"        if result is not None:",
			"            return result, best[0]",
			"",
			"def recursive_best_first_search(start, goal):",
			"    path, cost = rbfs(start, [start], 0, float(\"inf\"), goal)",
			"    return path, cost",
			"",
			"path, cost = recursive_best_first_search(\"S\", \"G\")",
			"print(\"RBFS Path:\", path, \"Cost:\", cost)"
		],
		"description": "Implements the Recursive Best-First Search (RBFS) algorithm."
	},
	"Decision Tree": {
		"prefix": "Decision Tree",
		"body": [
			"import pandas as pd",
			"import matplotlib.pyplot as plt",
			"import seaborn as sns",
			"from sklearn.datasets import load_iris",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.tree import DecisionTreeClassifier, plot_tree",
			"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix",
			"",
			"iris = load_iris()",
			"X = pd.DataFrame(iris.data, columns=iris.feature_names)",
			"y = pd.Series(iris.target, name=\"species\")",
			"",
			"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)",
			"",
			"dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)",
			"dtree.fit(X_train, y_train)",
			"",
			"y_pred = dtree.predict(X_test)",
			"print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))",
			"print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))",
			"",
			"cm = confusion_matrix(y_test, y_pred)",
			"plt.figure(figsize=(6,4))",
			"sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)",
			"plt.xlabel(\"Predicted\")",
			"plt.ylabel(\"Actual\")",
			"plt.title(\"Confusion Matrix\")",
			"plt.show()",
			"",
			"plt.figure(figsize=(12,8))",
			"plot_tree(dtree, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, fontsize=10)",
			"plt.show()"
		],
		"description": "Builds, evaluates, and visualizes a Decision Tree classifier on the Iris dataset."
	},
	"Feed Forward Neural Network From Scratch (NumPy)": {
		"prefix": "Feed Forward Neural Network From Scratch (NumPy)",
		"body": [
			"import numpy as np",
			"from sklearn.datasets import load_iris",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.preprocessing import OneHotEncoder, StandardScaler",
			"",
			"iris = load_iris()",
			"X, y = iris.data, iris.target.reshape(-1, 1)",
			"",
			"encoder = OneHotEncoder()",
			"y = encoder.fit_transform(y)",
			"",
			"scaler = StandardScaler()",
			"X = scaler.fit_transform(X)",
			"",
			"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
			"",
			"y_train = y_train.toarray()",
			"y_test = y_test.toarray()",
			"",
			"input_size = X_train.shape[1]",
			"hidden_size = 8",
			"output_size = y_train.shape[1]",
			"lr = 0.1",
			"epochs = 500",
			"",
			"np.random.seed(42)",
			"W1 = np.random.randn(input_size, hidden_size)",
			"b1 = np.zeros((1, hidden_size))",
			"W2 = np.random.randn(hidden_size, output_size)",
			"b2 = np.zeros((1, output_size))",
			"",
			"def sigmoid(x): return 1 / (1 + np.exp(-x))",
			"def sigmoid_deriv(x): return x * (1 - x)",
			"def softmax(x): return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)",
			"",
			"for epoch in range(epochs):",
			"    z1 = np.dot(X_train, W1) + b1",
			"    a1 = sigmoid(z1)",
			"    z2 = np.dot(a1, W2) + b2",
			"    a2 = softmax(z2)",
			"    loss = -np.mean(np.sum(y_train * np.log(a2 + 1e-9), axis=1))",
			"    dz2 = a2 - y_train",
			"    dW2 = np.dot(a1.T, dz2) / X_train.shape[0]",
			"    db2 = np.sum(dz2, axis=0, keepdims=True) / X_train.shape[0]",
			"    dz1 = np.dot(dz2, W2.T) * sigmoid_deriv(a1)",
			"    dW1 = np.dot(X_train.T, dz1) / X_train.shape[0]",
			"    db1 = np.sum(dz1, axis=0, keepdims=True) / X_train.shape[0]",
			"    W1 -= lr * dW1",
			"    b1 -= lr * db1",
			"    W2 -= lr * dW2",
			"    b2 -= lr * db2",
			"    if epoch % 100 == 0:",
			"        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")",
			"",
			"z1 = np.dot(X_test, W1) + b1",
			"a1 = sigmoid(z1)",
			"z2 = np.dot(a1, W2) + b2",
			"a2 = softmax(z2)",
			"y_pred = np.argmax(a2, axis=1)",
			"y_true = np.argmax(y_test, axis=1)",
			"accuracy = np.mean(y_pred == y_true)",
			"print(\"Test Accuracy (NumPy NN):\", accuracy)"
		],
		"description": "Implements a Feed Forward Neural Network from scratch using NumPy for the Iris dataset."
	},
	"Feed Forward Neural Network Using TensorFlow / Keras": {
		"prefix": "Feed Forward Neural Network Using TensorFlow / Keras",
		"body": [
			"import tensorflow as tf",
			"from tensorflow.keras import models, layers",
			"from sklearn.datasets import load_iris",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.preprocessing import OneHotEncoder, StandardScaler",
			"",
			"iris = load_iris()",
			"X, y = iris.data, iris.target.reshape(-1, 1)",
			"",
			"encoder = OneHotEncoder()",
			"y = encoder.fit_transform(y).toarray()",
			"",
			"scaler = StandardScaler()",
			"X = scaler.fit_transform(X)",
			"",
			"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
			"",
			"model = models.Sequential([",
			"    layers.Dense(8, input_shape=(4,), activation='sigmoid'),",
			"    layers.Dense(3, activation='softmax')",
			"])",
			"",
			"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",
			"",
			"history = model.fit(X_train, y_train, epochs=50, validation_split=0.1, verbose=2)",
			"",
			"loss, acc = model.evaluate(X_test, y_test, verbose=0)",
			"print(\"Test Accuracy (Keras NN):\", acc)"
		],
		"description": "Implements a Feed Forward Neural Network using TensorFlow/Keras for the Iris dataset."
	},
	"SVM": {
		"prefix": "SVM",
		"body": [
			"import numpy as np",
			"import matplotlib.pyplot as plt",
			"import seaborn as sns",
			"from sklearn.datasets import make_classification",
			"from sklearn.model_selection import train_test_split, GridSearchCV",
			"from sklearn.svm import SVC",
			"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix",
			"",
			"def plot_decision_boundary(model, X, y):",
			"    h = 0.02",
			"    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1",
			"    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1",
			"    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))",
			"    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])",
			"    Z = Z.reshape(xx.shape)",
			"    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)",
			"    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolors='k', cmap=plt.cm.coolwarm)",
			"    plt.xlabel(\"Feature 1\")",
			"    plt.ylabel(\"Feature 2\")",
			"    plt.title(\"SVM Decision Boundary\")",
			"    plt.show()",
			"",
			"def main():",
			"    # 1. Generate synthetic dataset",
			"    X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42)",
			"    ",
			"    # 2. Split into train/test sets",
			"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)",
			"    ",
			"    # 3. Define parameter grid for GridSearchCV",
			"    param_grid = {",
			"        'C': [0.1, 1, 10],",
			"        'kernel': ['linear', 'rbf', 'poly'],",
			"        'gamma': ['scale', 'auto']",
			"    }",
			"    ",
			"    # 4. Perform Grid Search with 5-fold cross-validation",
			"    grid = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy')",
			"    grid.fit(X_train, y_train)",
			"    print(\"Best Parameters:\", grid.best_params_)",
			"    ",
			"    # 5. Evaluate the best model found by Grid Search",
			"    best_svm = grid.best_estimator_",
			"    y_best_pred = best_svm.predict(X_test)",
			"    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_best_pred))",
			"    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_best_pred))",
			"    ",
			"    # 6. Visualize the confusion matrix",
			"    cm = confusion_matrix(y_test, y_best_pred)",
			"    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")",
			"    plt.xlabel(\"Predicted\")",
			"    plt.ylabel(\"Actual\")",
			"    plt.title(\"Confusion Matrix\")",
			"    plt.show()",
			"    ",
			"    # 7. Visualize the decision boundary",
			"    plot_decision_boundary(best_svm, X, y)",
			"",
			"if __name__ == '__main__':",
			"    main()"
		],
		"description": "Trains and evaluates a Support Vector Machine (SVM) classifier with hyperparameter tuning."
	},
	"Adaboost Ensemble Learning": {
		"prefix": "Adaboost Ensemble Learning",
		"body": [
			"import numpy as np",
			"from sklearn.datasets import make_classification",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.ensemble import AdaBoostClassifier",
			"from sklearn.tree import DecisionTreeClassifier",
			"from sklearn.metrics import accuracy_score, classification_report",
			"",
			"X, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=0, random_state=42)",
			"",
			"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
			"",
			"base_estimator = DecisionTreeClassifier(max_depth=1)",
			"",
			"adaboost = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)",
			"",
			"adaboost.fit(X_train, y_train)",
			"",
			"y_pred = adaboost.predict(X_test)",
			"",
			"print(\"Accuracy:\", accuracy_score(y_test, y_pred))",
			"print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
		],
		"description": "Implements the AdaBoost ensemble learning algorithm with Decision Tree stumps."
	},
	"Naive Bayes Classifier": {
		"prefix": "Naive Bayes Classifier",
		"body": [
			"from sklearn.datasets import load_iris",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.naive_bayes import GaussianNB",
			"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix",
			"",
			"iris = load_iris()",
			"X, y = iris.data, iris.target",
			"",
			"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
			"",
			"nb = GaussianNB()",
			"nb.fit(X_train, y_train)",
			"",
			"y_pred = nb.predict(X_test)",
			"y_prob = nb.predict_proba(X_test)",
			"",
			"print(\"Accuracy:\", accuracy_score(y_test, y_pred))",
			"print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))",
			"print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))",
			"",
			"for i in range(5):",
			"    print(f\"Sample {i+1} True={iris.target_names[y_test[i]]}, Pred={iris.target_names[y_pred[i]]}\")",
			"    print(\"Probabilities:\", y_prob[i])",
			"    print(\"-\" * 40)"
		],
		"description": "Implements and evaluates a Gaussian Naive Bayes classifier on the Iris dataset."
	},
	"K-Nearest Neighbors for Classification & Regression": {
		"prefix": "K-Nearest Neighbors for Classification & Regression",
		"body": [
			"import numpy as np",
			"import pandas as pd",
			"from sklearn.model_selection import train_test_split",
			"from sklearn.preprocessing import StandardScaler",
			"from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor",
			"from sklearn.metrics import accuracy_score, mean_squared_error",
			"from sklearn.datasets import load_iris, fetch_california_housing",
			"",
			"# Load Classification dataset (Iris)",
			"iris = load_iris()",
			"X_cls, y_cls = iris.data, iris.target",
			"",
			"# Load Regression dataset (California housing)",
			"housing = fetch_california_housing()",
			"X_reg, y_reg = housing.data, housing.target",
			"",
			"# Train-Test Split for Classification",
			"X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)",
			"",
			"# Train-Test Split for Regression",
			"X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)",
			"",
			"# Feature scaling",
			"scaler = StandardScaler()",
			"X_train_cls = scaler.fit_transform(X_train_cls)",
			"X_test_cls = scaler.transform(X_test_cls)",
			"X_train_reg = scaler.fit_transform(X_train_reg)",
			"X_test_reg = scaler.transform(X_test_reg)",
			"",
			"# KNN for Classification",
			"knn_cls = KNeighborsClassifier(n_neighbors=5)  # k = 5",
			"knn_cls.fit(X_train_cls, y_train_cls)",
			"y_pred_cls = knn_cls.predict(X_test_cls)",
			"acc = accuracy_score(y_test_cls, y_pred_cls)",
			"print(\"Classification Accuracy (Iris dataset):\", acc)",
			"",
			"# KNN for Regression",
			"knn_reg = KNeighborsRegressor(n_neighbors=5)  # k = 5",
			"knn_reg.fit(X_train_reg, y_train_reg)",
			"y_pred_reg = knn_reg.predict(X_test_reg)",
			"mse = mean_squared_error(y_test_reg, y_pred_reg)",
			"print(\"Regression MSE (California Housing):\", mse)",
			"",
			"# Result Analysis",
			"print(\"\\nResult Analysis:\")",
			"print(\"• Iris dataset classification accuracy is usually ~95–100% with k=5.\")",
			"print(\"• California housing regression MSE varies; smaller MSE indicates better performance.\")",
			"print(\"• You can tune 'k' (try odd numbers like 3, 5, 7, etc.) to improve results.\")"
		],
		"description": "Demonstrates K-Nearest Neighbors for both classification (Iris) and regression (California Housing)."
	},
	"Apriori Algorithm for Association Rule Mining (Practical on Market Basket Analysis).": {
		"prefix": "Apriori Algorithm for Association Rule Mining (Practical on Market Basket Analysis).",
		"body": [
			"import pandas as pd",
			"from mlxtend.frequent_patterns import apriori, association_rules",
			"from mlxtend.preprocessing import TransactionEncoder",
			"",
			"dataset = [",
			"    ['Milk', 'Bread', 'Eggs'],",
			"    ['Milk', 'Bread'],",
			"    ['Milk', 'Eggs'],",
			"    ['Bread', 'Eggs'],",
			"    ['Milk', 'Bread', 'Butter'],",
			"    ['Bread', 'Butter']",
			"]",
			"",
			"te = TransactionEncoder()",
			"te_array = te.fit(dataset).transform(dataset)",
			"df = pd.DataFrame(te_array, columns=te.columns_)",
			"print(\"One-hot encoded transaction data:\\n\")",
			"print(df)",
			"",
			"frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)",
			"print(\"\\nFrequent Itemsets:\\n\")",
			"print(frequent_itemsets)",
			"",
			"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)",
			"print(\"\\nAssociation Rules:\\n\")",
			"print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])",
			"",
			"print(\"\\nInterpretation Examples:\")",
			"for _, row in rules.iterrows():",
			"    antecedent = ', '.join(list(row['antecedents']))",
			"    consequent = ', '.join(list(row['consequents']))",
			"    print(f\"If {antecedent} is bought -> {consequent} is also bought \"",
			"          f\"{row['confidence']*100:.2f}% of the time (Lift={row['lift']:.2f}).\")"
		],
		"description": "Implements the Apriori algorithm for market basket analysis using mlxtend."
	},
	"TensorFlow Demo - Digit Classification with MNIST": {
		"prefix": "TensorFlow Demo - Digit Classification with MNIST",
		"body": [
			"import tensorflow as tf",
			"from tensorflow.keras import datasets, layers, models",
			"import matplotlib.pyplot as plt",
			"",
			"(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()",
			"x_train, x_test = x_train / 255.0, x_test / 255.0",
			"",
			"model = models.Sequential([",
			"    layers.Flatten(input_shape=(28, 28)),",
			"    layers.Dense(128, activation='relu'),",
			"    layers.Dense(10, activation='softmax')",
			"])",
			"",
			"model.compile(optimizer='adam',",
			"              loss='sparse_categorical_crossentropy',",
			"              metrics=['accuracy'])",
			"",
			"history = model.fit(x_train, y_train, epochs=5, validation_split=0.1, verbose=2)",
			"test_loss, test_acc = model.evaluate(x_test, y_test)",
			"print(\"\\nTest Accuracy:\", test_acc)",
			"",
			"plt.imshow(x_test[0], cmap=\"gray\")",
			"plt.title(f\"Predicted: {model.predict(x_test[:1]).argmax()}\")",
			"plt.show()"
		],
		"description": "Trains a simple neural network to classify handwritten digits from the MNIST dataset using TensorFlow."
	}
}

